{"name":"Openstack-automation","tagline":"Openstack deployment using saltstack","body":"openstack-automation\r\n====================\r\n\r\nOpenstack deployment using saltstack\r\n\r\nThere are several methods for automated deployment of openstack cluster. In this blog we attempt to do it using [saltstack](http://docs.saltstack.com/ \"Saltstack\"). There is almost no coding involved and it can be easily maintained. Above all it is as easy as talking to your servers and asking them to configure themselves. \r\n\r\nSaltstack provides us an infrastructure management framework that makes our job easier. Saltstack supports most of tasks that you would want to perform while installing openstack and more. We might not need any programming to do so. All we do need to do is define our cluster as below.\r\n\r\n<pre>\r\ncluster_entities: \r\n  - compute\r\n  - controller\r\n  - network\r\ncompute: \r\n  - venus\r\ncontroller: \r\n  - mercury\r\nnetwork: \r\n  - mercury\r\nneutron: \r\n  metadata_secret: 414c66b22b1e7a20cc35\r\n  intergration_bridge: br-int\r\n  network_mode: vlan\r\n  venus: \r\n    Intnet1: \r\n      start_vlan: 100\r\n      end_vlan: 200\r\n      bridge: br-eth1\r\n      interface: eth1\r\n  mercury: \r\n    Intnet1: \r\n      start_vlan: 100\r\n      end_vlan: 200\r\n      bridge: br-eth1\r\n      interface: eth1\r\n    Extnet: \r\n      bridge: br-ex\r\n      interface: eth2\r\ninstall: \r\n  controller: \r\n    - generics.havana_cloud_repo\r\n    - generics.apt-proxy\r\n    - generics.headers\r\n    - generics.host\r\n    - mysql\r\n    - mysql.client\r\n    - mysql.openstack_dbschema\r\n    - queue.rabbit\r\n    - keystone\r\n    - keystone.openstack_tenants\r\n    - keystone.openstack_users\r\n    - keystone.openstack_services\r\n    - glance\r\n    - nova\r\n    - horizon\r\n  network: \r\n    - generics.havana_cloud_repo\r\n    - generics.apt-proxy\r\n    - generics.headers\r\n    - generics.host\r\n    - neutron\r\n    - neutron.openvswitch\r\n  compute: \r\n    - generics.havana_cloud_repo\r\n    - generics.apt-proxy\r\n    - generics.headers\r\n    - generics.host\r\n    - nova.compute_kvm\r\n    - neutron.openvswitch\r\nhosts: \r\n  mercury: mercury_ip_here\r\n  venus: venus_ip_here\r\n  saturn: saturn_ip_here\r\n  salt: sat_master_ip_here\r\n\r\n</pre>\r\n\r\nWhat we saw above is the [pillar](http://docs.saltstack.com/topics/pillar/ \"Salt Pillar\") definition. Should you need to change your cluster definition you do so by changing the pillar and synchronising the changes. Although this file defines your cluster entirely the file in itself can do nothing. The entire project has been checked in [here](https://github.com/Akilesh1597/openstack-automation/ \"Openstack-Automation\").\r\n\r\nTo test it create a new [environment](http://docs.saltstack.com/ref/file_server/index.html#environments \"Salt Environments\") in your salt master configuration file and point it to where you have downloaded the project like so.\r\n\r\n<pre>\r\n\r\nfile_roots:\r\n   base:\r\n     - /srv/salt/\r\n   icehouse:\r\n     - /srv/icehouse/file\r\n\r\npillar_roots:\r\n  base:\r\n    - /srv/pillar\r\n  icehouse:\r\n    - /srv/icehouse/pillar\r\n\r\n</pre>\r\n\r\nThis will add the havana environment to your saltstack. The 'file_roots' will have [state definitions](http://docs.saltstack.com/topics/tutorials/starting_states.html \"Salt States\") in a bunch of '.sls' files and the few special directories, while the 'pillar_roots' has your cluster definition. \r\n\r\nAt this stage I assume that you have two machines 'mercury' and 'venus' (these are their hostnames as well as their minion id) added to the salt master. See [setting up salt minion](http://docs.saltstack.com/topics/tutorials/walkthrough.html#setting-up-a-salt-minion \"minion setup\") to add minions to the master. The pillar definition file shown at the begining has a key 'cluster_type'. We will use this key to deliver commands to the two machines alone like so.\r\n\r\n<pre>\r\nsalt '*' saltutil.sync_all\r\nsalt '*' saltutil.refresh_pillar\r\nsalt -C 'I@cluster_type:openstack' state.highstate\r\n</pre>\r\n\r\n\r\nThis instructs all the minions those who have 'cluster_type=openstack' defined in their pillar data to download all the state defined for them and execute the same. If all goes well you can login to your newly installed openstack setup 'http://mercury/horizon'.\r\n\r\n\r\nNote\r\n====\r\nThe state generics.apt-proxy installs a file at /etc/apt/apt.conf.d/01proxy. This points to your salt master as [apt cache proxy](https://help.ubuntu.com/community/Apt-Cacher-Server \"Apt Cache\"). If you do not want this to happen delete the file '/etc/apt/apt.conf.d/01proxy' and remove the formula \"generics.apt-proxy\" from your pillar config file.\r\n\r\n\r\nCluster Definition\r\n==================\r\n\r\nTo make things clear lets have a look at a part of the pillar configuration.\r\n\r\n<pre>\r\n  \r\ncluster_entities: \r\n  - compute\r\n  - controller\r\n  - network\r\ncompute: \r\n  - venus\r\ncontroller: \r\n  - mercury\r\nnetwork: \r\n  - mercury\r\ninstall: \r\n  controller: \r\n    - generics.havana_cloud_repo\r\n    - generics.apt-proxy\r\n    - generics.headers\r\n    - generics.host\r\n    - mysql\r\n    - mysql.client\r\n    - mysql.openstack_dbschema\r\n    - queue.rabbit\r\n    - keystone\r\n    - keystone.openstack_tenants\r\n    - keystone.openstack_users\r\n    - keystone.openstack_services\r\n    - glance\r\n    - nova\r\n    - horizon\r\n  network: \r\n    - generics.havana_cloud_repo\r\n    - generics.apt-proxy\r\n    - generics.headers\r\n    - generics.host\r\n    - neutron\r\n    - neutron.openvswitch\r\n  compute: \r\n    - generics.havana_cloud_repo\r\n    - generics.apt-proxy\r\n    - generics.headers\r\n    - generics.host\r\n    - nova.compute_kvm\r\n    - neutron.openvswitch\r\n</pre>\r\n\r\nThis \"cluster_entities\" defines what are the entities that form the cluster. The controller node, network node and compute node form the entities of an openstack cluster.\r\n\r\nThen we define which machine performs the role of each of the defined entities.\r\n\r\nFinally the “install” section defines what formulas to apply on a machine in order to deploy each of the entities.\r\n\r\n\r\n\r\nAdding a new cluster entity\r\n===========================\r\n\r\nLets say we need add a new entity called queue_server which will run rabbitmq. This is how we do it.\r\n1. Add a new entitiy \"queue_server\" under \"cluster_entities\"\r\n2. Define what minions will perform the role of \"queue_server\"\r\n3. Finally define which formula deploys a \"queue_server\" under the \"install\".\"queue_server\" section.\r\n<pre>\r\n \r\ncluster_entities: \r\n  - compute\r\n  - controller\r\n  - network\r\n  - queue_server\r\ncompute: \r\n  - venus\r\ncontroller: \r\n  - mercury\r\nnetwork: \r\n  - mercury\r\nqueue_server: \r\n  - jupiter\r\ninstall: \r\n  controller: \r\n    - generics.havana_cloud_repo\r\n    - generics.apt-proxy\r\n    - generics.headers\r\n    - generics.host\r\n    - mysql\r\n    - mysql.client\r\n    - mysql.openstack_dbschema\r\n    - queue.rabbit\r\n    - keystone\r\n    - keystone.openstack_tenants\r\n    - keystone.openstack_users\r\n    - keystone.openstack_services\r\n    - glance\r\n    - nova\r\n    - horizon\r\n  network: \r\n    - generics.havana_cloud_repo\r\n    - generics.apt-proxy\r\n    - generics.headers\r\n    - generics.host\r\n    - neutron\r\n    - neutron.openvswitch\r\n  compute: \r\n    - generics.havana_cloud_repo\r\n    - generics.apt-proxy\r\n    - generics.headers\r\n    - generics.host\r\n    - nova.compute_kvm\r\n    - neutron.openvswitch\r\n  compute: \r\n    - generics.havana_cloud_repo\r\n    - generics.apt-proxy\r\n    - generics.headers\r\n    - generics.host\r\n    - nova.compute_kvm\r\n    - neutron.openvswitch\r\n  queue_server:\r\n    - queue.rabbit\r\n</pre>\r\n\r\nThen edit your 'pillar/top.sls' and point jupiter to use 'openstack_cluster.sls'\r\n\r\n<pre>\r\nicehouse:\r\n  mercury:\r\n    - openstack_cluster\r\n  venus:\r\n    - openstack_cluster\r\n  jupiter:\r\n    - openstack_cluster\r\n</pre>\r\n\r\nThen sync up the cluster as shown below.\r\n\r\n<pre>\r\nsalt '*' saltutil.sync_all\r\nsalt '*' saltutil.refresh_pillar\r\nsalt -C 'I@cluster_type:openstack' state.highstate\r\n</pre>\r\n\r\n\r\nAdding new compute node\r\n=======================\r\n\r\nAdding a new machine to a cluster is as easy as editing a json file. All you have to do is edit 'pillar/openstack_cluster.sls' as below.\r\n\r\n<pre>\r\ncompute:\r\n  - venus\r\n  - saturn\r\n</pre>\r\n\r\nThen edit your 'pillar/top.sls' and point saturn to use 'openstack_cluster.sls'\r\n\r\n<pre>\r\nhavana:\r\n  mercury:\r\n   - openstack_cluster\r\n  venus:\r\n   - openstack_cluster\r\n  jupiter:\r\n   - openstack_cluster\r\n  saturn:\r\n   - openstack_cluster\r\n</pre>\r\n\r\nThen sync up the cluster as shown below.\r\n\r\n<pre>\r\nsalt '*' saltutil.sync_all\r\nsalt '*' saltutil.refresh_pillar\r\nsalt -C 'I@cluster_type:openstack' state.highstate\r\n</pre>\r\n\r\n\r\nOVS Vlan mode networking\r\n========================\r\n\r\nThe default configuration will install a vlan mode network. Have a close look at the configuration.\r\n\r\n<pre>\r\nneutron: \r\n  metadata_secret: 414c66b22b1e7a20cc35\r\n  intergration_bridge: br-int\r\n  network_mode: vlan\r\n  venus: \r\n    Intnet1: \r\n      start_vlan: 100\r\n      end_vlan: 200\r\n      bridge: br-eth1\r\n      interface: eth1\r\n  mercury: \r\n    Intnet1: \r\n      start_vlan: 100\r\n      end_vlan: 200\r\n      bridge: br-eth1\r\n      interface: eth1\r\n    Extnet: \r\n      bridge: br-ex\r\n      interface: eth2\r\n</pre>\r\nFor each node \"venus\", \"mercury\" etc you specify the physical_networks, the start and end vlan id in that physnet, the bridge that is connected to the physnet and the interface that should be present in the brdige. For flat network do not provide any start and end vlan. \r\n\r\n\r\nOVS GRE mode networking\r\n=======================\r\n\r\nIf GRE mode networking is desired please alter the pillar file as below.\r\n\r\n<pre>\r\nneutron: \r\n  metadata_secret: 414c66b22b1e7a20cc35\r\n  intergration_bridge: br-int\r\n  network_mode: tunnel\r\n  tunnel_start: 100\r\n  tunnel_end: 200\r\n  tunnel_type: gre\r\n</pre>\r\n\r\n\r\nOVS VXLAN mode networking\r\n=========================\r\n\r\nIf VXLAN mode networking is desired please alter the pillar file as below.\r\n\r\n<pre>\r\n  metadata_secret: 414c66b22b1e7a20cc35\r\n  intergration_bridge: br-int\r\n  network_mode: tunnel\r\n  tunnel_start: 100\r\n  tunnel_end: 200\r\n  tunnel_type: vxlan\r\n</pre>\r\n\r\n\r\n\r\n\r\nRemoving a node\r\n===============\r\n\r\nChange the file 'pillar_root/top.sls' from \r\n\r\n<pre>\r\nhavana:\r\n  hawk:\r\n    - openstack_cluster\r\n  lammer:\r\n    - openstack_cluster\r\n</pre>\r\nto\r\n<pre>\r\nhavana:\r\n  hawk:\r\n    - openstack_cluster_inverse\r\n  lammer:\r\n    - openstack_cluster_inverse\r\n</pre>\r\n\r\nThen sync up the cluster as always.\r\n\r\n<pre>\r\nsalt '*' saltutil.sync_all\r\nsalt -C 'I@cluster_type:openstack' state.highstate\r\n</pre>\r\n\r\nNote\r\n====\r\nThis is not tested completely and it will remove only the packages from the hosts. Detaching the host from openstack controller is still a manual task. \r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}